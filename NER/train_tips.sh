"""
Articles: https://levelup.gitconnected.com/auto-detect-anything-with-custom-named-entity-recognition-ner-c89d6562e8e9
Step 1, go to https://spacy.io/usage/training#config, select cpu/gpu/ner etc, copy contents to create base_config.cfg
Step 2, modify base_config.cfg to fill in the train and dev datasets (e.g., TRAIN.spacy and DEV.spacy)
Step 3, generate config.cfg using command line: python -m spacy init fill-config base_config.cfg config.cfg
Step 4, train model by: python -m spacy train config.cfg --output ./outpath; for gpu add -g 0
"""
# conda deactivate to exit the <base> environment

# create config.cfg from base_config.cfg
python -m spacy init fill-config base_config.cfg config.cfg

# train with an output folder
# need to install cupy that matches cuda (see nvidia-smi): pip install -U spacy[cuda114]
python -m spacy train config.cfg --output ./model --gpu-id 0    # or -g 0

"""
It appears CPU train with 'tok2vec' (en_core_web_lg) outperforms GPU train with 'transformer' (roberta-base)

CPU train iterations:
E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  ------------  --------  ------  ------  ------  ------
  0       0          0.00    200.60    0.00    0.00    0.00    0.00
  0     200        638.45   7972.27   67.99   76.32   61.30    0.68
  0     400        832.01   3172.61   85.89   85.90   85.87    0.86
  0     600        704.98   1703.83   88.76   93.05   84.85    0.89
  0     800        235.43   1449.28   88.29   89.29   87.32    0.88
  0    1000        336.63   1215.21   94.27   94.38   94.16    0.94
  0    1200        561.97    937.63   94.06   94.39   93.72    0.94
  0    1400        235.36    721.03   93.88   93.42   94.35    0.94
  0    1600        379.76    821.78   95.53   96.82   94.28    0.96
  0    1800        392.68    680.66   95.97   96.74   95.22    0.96
  0    2000        395.24    599.09   96.96   97.55   96.37    0.97
  0    2200      30745.03    561.62   97.47   98.45   96.52    0.97
  1    2400      67151.08    585.48   97.11   97.41   96.80    0.97
  1    2600      18736.06    497.72   97.97   98.11   97.83    0.98
  1    2800        799.90    370.64   97.08   97.71   96.46    0.97
  1    3000        574.53    268.20   96.83   98.41   95.31    0.97
  1    3200       4289.05    430.77   96.75   97.53   96.00    0.97
  1    3400     725499.58    731.68   97.88   97.64   98.12    0.98
  1    3600       1757.81    352.36   98.50   98.17   98.82    0.98
  1    3800       1658.43    442.23   95.71   98.27   93.28    0.96
  1    4000       1175.95    388.66   98.88   99.07   98.69    0.99
  1    4200       1013.38    291.06   97.48   97.08   97.88    0.97
  1    4400        751.46    274.82   98.58   98.51   98.66    0.99
  2    4600      68967.63    553.56   98.39   98.01   98.77    0.98
  2    4800       2037.44    291.63   98.48   98.53   98.43    0.98
  2    5000        524.04    176.82   97.85   97.83   97.86    0.98
  2    5200        892.07    285.07   98.77   99.10   98.45    0.99
  2    5400       1371.95    281.42   98.59   98.71   98.48    0.99
  2    5600       1100.51    254.61   97.54   96.71   98.39    0.98
✔ Saved pipeline to output directory

GPU train iterations:  
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0        1290.69   1416.62    0.18    0.10    1.11    0.00
  0     200      127586.71  92811.92   70.27   73.66   67.17    0.70
  0     400       64715.19  14152.88   90.34   88.50   92.25    0.90
  1     600        9511.94   5264.00   94.55   91.37   97.95    0.95
  1     800        7306.38   4136.82   94.32   91.40   97.45    0.94
  2    1000       17924.64   3817.41   95.16   91.92   98.64    0.95
  2    1200       12356.89   3271.11   94.60   91.79   97.59    0.95
  3    1400       24701.54   3997.72   95.65   92.60   98.91    0.96
  3    1600       49424.62   3820.56   95.52   92.34   98.92    0.96
  4    1800       26901.67   3174.91   95.18   92.22   98.34    0.95
  4    2000       37272.09   3546.82   95.77   92.57   99.21    0.96
  5    2200       10345.99   2884.97   95.87   92.61   99.37    0.96
  5    2400        5206.44   2507.98   95.56   92.53   98.80    0.96
  6    2600       30641.32   2762.21   95.78   92.68   99.09    0.96
  6    2800       48905.62   3287.29   95.68   92.62   98.95    0.96
  7    3000        7005.15   2808.00   96.01   92.71   99.55    0.96
  7    3200       11927.43   2897.51   95.65   92.44   99.09    0.96
  8    3400       55500.99   2769.51   96.02   92.71   99.57    0.96
  8    3600         814.04   2373.61   96.04   92.77   99.54    0.96
  9    3800       23523.08   2467.98   95.98   92.59   99.63    0.96
  9    4000       66284.86   2624.28   96.06   92.70   99.67    0.96
 10    4200      144681.89   5282.53   95.72   92.68   98.97    0.96
 10    4400         709.85   2375.32   96.06   92.82   99.52    0.96
 11    4600         319.27   2223.13   95.83   92.66   99.22    0.96
 11    4800       29890.78   2242.26   96.00   92.79   99.43    0.96
 11    5000        1125.63   2417.79   96.14   92.89   99.63    0.96
 12    5200         950.77   2241.30   96.02   92.83   99.43    0.96
 12    5400        3698.00   2537.60   96.01   92.64   99.64    0.96
 13    5600      108130.35   2889.75   95.96   92.63   99.54    0.96
 13    5800      192854.70   2591.61   95.97   92.63   99.57    0.96
 14    6000       33095.75   2191.77   95.90   92.89   99.12    0.96
 14    6200        2644.21   2155.34   99.59   99.57   99.61    1.00
 15    6400        1303.90   2075.01   99.58   99.57   99.60    1.00
 15    6600      139807.18   2280.50   99.65   99.66   99.64    1.00
 16    6800       15587.92   2350.28   99.34   99.59   99.09    0.99
 16    7000        7590.45   2019.40   99.57   99.57   99.57    1.00
 17    7200         599.74   2110.04   99.49   99.32   99.66    0.99
 17    7400         430.69   1820.17   99.32   99.53   99.10    0.99
 18    7600        1241.88   1986.53   99.47   99.39   99.55    0.99
 18    7800       36956.51   1901.11   99.58   99.64   99.52    1.00
 19    8000         779.22   1801.74   99.46   99.29   99.64    0.99
 19    8200      353210.21   2268.87   99.51   99.36   99.66    1.00
✔ Saved pipeline to output directory
"""

